{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Medical Library"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = soup.fetch('a' , {'class' : 'bold'})\n",
      "href_dict = {}\n",
      "i = 5\n",
      "while i < len(a):\n",
      "    href_dict['http://medlib.bu.edu/webcollections/' + a[i]['href']] = 'url'\n",
      "    i += 1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "os.chdir("
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subjects = soup.find('a', {'name' : 'print'})\n",
      "next = subjects.findNextSibling()\n",
      "if next.contents[0] ==  'Print Resources':\n",
      "    next = next.findNextSibling()\n",
      "    a = next.a.contents[0]\n",
      "    print a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Research\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"subjects.contents[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 78,
       "text": [
        "u'Research, Biomedical'"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = subjects.findNextSibling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test.a.contents[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 80,
       "text": [
        "u'Clinical Trials'"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Medical Library\n",
      "##\n",
      "## Variables:\n",
      "##\n",
      "## search_url contains the initial search string. In this case we\n",
      "## are searching for all of the guides published by the BU Libraries\n",
      "## on the Libraries' web site:  * inurl:www.bu.edu/library/guide/\n",
      "search_url = 'http://medlib.bu.edu/webcollections/medsubs.php?start=A&stop=ANY'\n",
      "##\n",
      "## results_url contains the results of the initial search. To this string\n",
      "## we will append the total number of results parsed from the initial\n",
      "## results page.\n",
      "#results_url = 'http://www.bu.edu/phpbin/search/index.php?q=*+inurl:www.bu.edu/sthlibrary/library-research-guides/&start=0&col=default_collection&site=&t=default&sort=&dir=1&maps=1&num='\n",
      "##\n",
      "## If test is True, print the values ; If test is False, write to file\n",
      "test = False\n",
      "\n",
      "## Import the required libraries\n",
      "import os\n",
      "import urllib2\n",
      "from pymarc import Field, Record\n",
      "from BeautifulSoup import BeautifulSoup\n",
      "import cgi\n",
      "import tarfile\n",
      "# setup a variable for excaping HTML chars\n",
      "html_escape_table = {\n",
      "                     \"&\": \"&amp;\",\n",
      "                     '\"': \"&quot;\",\n",
      "                     \"'\": \"&apos;\",\n",
      "                     \">\": \"&gt;\",\n",
      "                     \"<\": \"&lt;\",\n",
      "                     }\n",
      "## define a function that will escape strings of text for use in xml\n",
      "def html_escape(text):\n",
      "    \"\"\"Produce entities within text.\"\"\"\n",
      "    return \"\".join(html_escape_table.get(c,c) for c in text) \n",
      "\n",
      "if not test:\n",
      "    ## open the xml file and write the header\n",
      "    f = open('med_guides.xml', 'wb')\n",
      "    f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
      "    f.write('<OAI-PMH xmlns=\"http://www.openarchives.org/OAI/2.0/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd\">\\n')\n",
      "    f.write('<responseDate>2014-07-09T21:02:57Z</responseDate>\\n')\n",
      "    f.write('    <request verb=\"ListRecords\" metadataPrefix=\"oai_dc\" >http://www.bu.edu/library/guides</request>\\n')\n",
      "    f.write('<ListRecords>\\n')\n",
      "\n",
      "\n",
      "\n",
      "## Get the full list of guides\n",
      "page = urllib2.urlopen(search_url)\n",
      "rpage = page.read()\n",
      "soup =  BeautifulSoup(rpage)\n",
      "## grab all of the 'a' tags\n",
      "a = soup.fetch('a' , {'class' : 'bold'})\n",
      "href_dict = {}\n",
      "i = 5\n",
      "while i < len(a):\n",
      "    href_dict['http://medlib.bu.edu/webcollections/' + a[i]['href']] = 'url'\n",
      "    i += 1\n",
      "\n",
      "href_list = list(href_dict.keys())\n",
      "\n",
      "##\n",
      "counter = 1\n",
      "for href in href_list:\n",
      "    page = urllib2.urlopen(href)\n",
      "    rpage = page.read()\n",
      "    info = page.info()\n",
      "    date = info['date']\n",
      "    date = date[5:-12]\n",
      "    soup =  BeautifulSoup(rpage)\n",
      "    ## get the title\n",
      "    title = soup.title.contents[0]\n",
      "    title = title[24:]\n",
      "\n",
      "    ## author\n",
      "    author = ''\n",
      "    authors = soup.findAll('div', {'class' : 'widget bu-library-profiles-widget guide_authors'})\n",
      "    if len(authors) > 0 :\n",
      "        authors = authors[0]\n",
      "        authors_list = authors.fetch('h3')\n",
      "        for a in authors_list:\n",
      "            if len(author) > 0:\n",
      "                author += '; '\n",
      "        author += a.a.contents[0]\n",
      "    ## subjects\n",
      "    subs = ''\n",
      "    subjects = soup.find('div', {'class' : 'header'})\n",
      "    subs = subjects.contents[0]\n",
      "    ##print 'Subjects: ', subs\n",
      "\n",
      "    url = href\n",
      "\n",
      "        \n",
      "    metaList = soup.fetch('meta')\n",
      "    meta_desc = ''\n",
      "    description = ''\n",
      "    meta_keywords = ''\n",
      "    meta_rights = 'Copyright (c) 2014 Boston University'\n",
      "    for l in metaList:\n",
      "        meta = l.attrs\n",
      "\n",
      "\n",
      "\n",
      "        if len(meta) == 2:\n",
      "            if meta[0][1] == 'description':\n",
      "                #print 'DESCRIPTION'\n",
      "                #print meta[1][1]\n",
      "                meta_desc = meta[1][1]\n",
      "                description = meta[1][1]\n",
      "                #print description\n",
      "                #print 'meta_desc:  ', meta_desc  \n",
      "            if meta[0][1] == 'keywords':\n",
      "                meta_keywords = meta[1][1]\n",
      "                #print 'meta_key:  ', meta_keywords  \n",
      "            #if meta[0][1] == 'copyright':\n",
      "             #   meta_rights = meta[1][1]\n",
      "                #print 'meta_rights:  ', meta_rights\n",
      "    if not test:\n",
      "        f.write('<record>\\n')\n",
      "        f.write('<header>\\n')\n",
      "        f.write('<identifier>\\n')\n",
      "        #id = d.entries[i].id\n",
      "        id = 'theolibguide:' + str(counter)\n",
      "        f.write(html_escape(id))\n",
      "        f.write('</identifier>\\n')\n",
      "        f.write('<datestamp>2009-09-19T06:00:13Z</datestamp>\\n')\n",
      "        f.write('</header>\\n')\n",
      "        f.write('<metadata>\\n')\n",
      "        f.write('<oai_dc:dc xmlns:oai_dc=\"http://www.openarchives.org/OAI/2.0/oai_dc/\" xmlns:doc=\"http://www.lyncode.com/xoai\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xsi:schemaLocation=\"http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd\">\\n')\n",
      "        f.write('<dc:title>')\n",
      "        f.write(html_escape(title.encode('utf-8')))\n",
      "        f.write('</dc:title>\\n')\n",
      "        if len(author) > 0:\n",
      "            f.write('<dc:creator>')\n",
      "            f.write(html_escape(author.encode('utf-8')))\n",
      "            f.write('</dc:creator>\\n')  \n",
      "        if len(subs) > 0:\n",
      "            f.write('<dc:subject>')\n",
      "            f.write(html_escape(subs.encode('utf-8')))\n",
      "            f.write('</dc:subject>\\n')\n",
      "        f.write('<dc:identifier>')\n",
      "        f.write(url)\n",
      "        f.write('</dc:identifier>\\n') \n",
      "        f.write('<dc:date>')\n",
      "        f.write(date)\n",
      "        f.write('</dc:date>') \n",
      "        f.write('<dc:subject>')\n",
      "        f.write('research guide')\n",
      "        f.write('</dc:subject>\\n')\n",
      "        if len(meta_desc) > 0:\n",
      "            f.write('<dc:description>')\n",
      "            f.write(html_escape(meta_desc.encode('utf-8')))\n",
      "            f.write('</dc:description>\\n') \n",
      "        if len(meta_keywords) > 0:\n",
      "            f.write('<dc:subject>')\n",
      "            f.write(html_escape(meta_keywords.encode('utf-8')))\n",
      "            f.write('</dc:subject>\\n') \n",
      "        f.write('</oai_dc:dc>\\n</metadata>\\n</record>\\n')\n",
      "        counter += 1\n",
      "    if test:\n",
      "        # lets see what we have\n",
      "        print 'record: ', str(counter)\n",
      "        print 'title: ', title\n",
      "        print 'author: ', author\n",
      "        print 'subject: ', subs\n",
      "        print 'keywords:', meta_keywords\n",
      "        print 'description: ', meta_desc\n",
      "        print 'link: ', url\n",
      "        print 'rights: ', meta_rights\n",
      "        print 'date: ', date\n",
      "        print ' '\n",
      "        counter += 1\n",
      "if not test:\n",
      "    f.write('</ListRecords>\\n')\n",
      "    f.write('</OAI-PMH>\\n')\n",
      "    f.close()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}